{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9046b528",
   "metadata": {},
   "source": [
    "# Crea un agente RAG con LangChain.\n",
    "\n",
    "Estudiante: Camilo Andr√©s Quintero Rodr√≠guez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea99550",
   "metadata": {},
   "source": [
    "## 1. Instalaci√≥n y configuraci√≥n del entorno\n",
    "\n",
    "Se instalan las dependencias necesarias para trabajar con **LangChain**, los modelos de lenguaje y el procesamiento de texto:\n",
    "\n",
    "```bash\n",
    "pip install langchain langchain-text-splitters langchain-community bs4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e33cfff",
   "metadata": {},
   "source": [
    "## 2. Configuraci√≥n de variables de entorno para LangSmith\n",
    "\n",
    "En este paso se configuran las **variables de entorno** necesarias para habilitar el seguimiento y monitoreo de las ejecuciones del agente dentro de **LangSmith**, una herramienta de observabilidad para LangChain.  \n",
    "\n",
    "El c√≥digo usa el m√≥dulo `os` para establecer valores que el sistema puede leer durante la ejecuci√≥n del programa.\n",
    "\n",
    "```bash\n",
    "pip install langchain langsmith\n",
    "pip install -U langchain langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc40bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe1a0a3",
   "metadata": {},
   "source": [
    "## 3. Inicializaci√≥n del modelo de lenguaje (LLM)\n",
    "\n",
    "En este paso se configura el **modelo de lenguaje** que el agente RAG utilizar√° para generar y razonar sobre las respuestas.  \n",
    "Se usa el modelo **Gemini 2.5 Flash Lite** de Google, accedido a trav√©s de la integraci√≥n de **LangChain**.\n",
    "\n",
    "```bash\n",
    "pip install langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbc1192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
    "\n",
    "model = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5175cc5",
   "metadata": {},
   "source": [
    "## 4. Creaci√≥n del almac√©n vectorial y configuraci√≥n de embeddings\n",
    "\n",
    "En este paso se prepara el sistema para **almacenar y recuperar informaci√≥n sem√°ntica**.  \n",
    "Esto es fundamental en un agente RAG (Retrieval-Augmented Generation), ya que permite **buscar fragmentos relevantes de texto** antes de generar una respuesta.\n",
    "\n",
    "```bash\n",
    "pip install langchain langchain-community langchain-text-splitters langchain-google-genai bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c92e991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d5117e",
   "metadata": {},
   "source": [
    "## 5. Carga y limpieza del documento fuente\n",
    "\n",
    "En este paso se **extrae contenido de una p√°gina web** para usarlo como base de conocimiento del agente RAG.  \n",
    "Se utiliza el cargador `WebBaseLoader` de **LangChain Community**, junto con **BeautifulSoup (bs4)** para limpiar el HTML y conservar solo la informaci√≥n relevante del art√≠culo.\n",
    "\n",
    "```bash\n",
    "pip install langchain-community bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "614d6fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Documento cargado correctamente.\n",
      "üìÑ Total de caracteres extra√≠dos: 43047\n",
      "\n",
      "üîπ Fragmento del texto extra√≠do:\n",
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "# step1_load_data.py\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Solo extrae el contenido relevante de la p√°gina\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "\n",
    "# Cargar el art√≠culo del blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "\n",
    "# Descargar y limpiar el contenido\n",
    "docs = loader.load()\n",
    "\n",
    "# Validar que se haya cargado correctamente\n",
    "assert len(docs) == 1, \"No se pudo cargar el documento desde la URL.\"\n",
    "\n",
    "# Mostrar informaci√≥n b√°sica\n",
    "print(f\"‚úÖ Documento cargado correctamente.\")\n",
    "print(f\"üìÑ Total de caracteres extra√≠dos: {len(docs[0].page_content)}\")\n",
    "\n",
    "# (opcional) Muestra los primeros 500 caracteres\n",
    "print(\"\\nüîπ Fragmento del texto extra√≠do:\")\n",
    "print(docs[0].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548b17df",
   "metadata": {},
   "source": [
    "## 6. Divisi√≥n del texto en fragmentos (chunks)\n",
    "\n",
    "En este paso se divide el texto cargado en **fragmentos m√°s peque√±os**, lo cual es esencial para el funcionamiento del modelo RAG.  \n",
    "Los modelos de lenguaje y los sistemas de b√∫squeda vectorial trabajan mejor cuando el texto est√° segmentado en partes manejables.\n",
    "\n",
    "```bash\n",
    "pip install langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d561204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 63 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49ab74d",
   "metadata": {},
   "source": [
    "## 7. Creaci√≥n de embeddings y almacenamiento vectorial\n",
    "\n",
    "En este paso se **convierte cada fragmento de texto en un vector num√©rico** (embedding) y se almacena en una base de datos vectorial en memoria.  \n",
    "Esto permite que el agente RAG busque informaci√≥n relevante de forma eficiente mediante **b√∫squeda sem√°ntica**.\n",
    "\n",
    "```bash\n",
    "pip install sentence-transformers langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6dbdf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Milo\\AppData\\Local\\Temp\\ipykernel_68\\105888337.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d73e1dcd79c4a15a003398972ad08db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Milo\\Documents\\RAG-LangChain\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Milo\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4edc71961ca24669bdc695d365780b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab340c80af444f229dd7ecc06022a4f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9d963c81714aa59c2e83f322cbbc07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6722e9c99641d6b6f4e76250e58151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac864aaec0d740b09fda4045a3af52dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c53c70755f384a918a90ba6f6b201a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb46cfbf1bf4d2bab7cce0b898afca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd54e82326145eca874dda84799579d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb546d9b73a4465b4f44a99030f3cb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54bbeb359b14e479dfd8a67f21f7e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_store = InMemoryVectorStore(embeddings) \n",
    "document_ids = vector_store.add_documents(documents=all_splits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868aa29b",
   "metadata": {},
   "source": [
    "## 8. Verificaci√≥n de los documentos almacenados\n",
    "\n",
    "Para confirmar que los fragmentos de texto fueron correctamente convertidos en vectores y guardados en la base de datos, se imprime una muestra de los **IDs** generados:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "012a5d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bd2df2a3-d978-4aad-b579-8d4239761924', 'e52acb44-c86d-47d5-9789-bcbe456912cd', '553e5c8b-267e-4726-a638-b4533722f02c']\n"
     ]
    }
   ],
   "source": [
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c87896",
   "metadata": {},
   "source": [
    "## 9. Creaci√≥n de una herramienta para recuperar contexto\n",
    "\n",
    "En este paso se define una funci√≥n que permitir√° buscar informaci√≥n relevante dentro de los documentos vectorizados, con el objetivo de **proveer contexto** a las consultas que se realicen al modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dca2cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35871d65",
   "metadata": {},
   "source": [
    "## 10. Creaci√≥n del agente RAG\n",
    "\n",
    "En este paso se crea el **agente inteligente (RAG)** que usar√° el modelo de lenguaje junto con la herramienta de b√∫squeda para responder preguntas basadas en el contenido del documento cargado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce6f8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "tools = [retrieve_context]\n",
    "# If desired, specify custom instructions\n",
    "prompt = (\n",
    "    \"You have access to a tool that retrieves context from a blog post. \"\n",
    "    \"Use the tool to help answer user queries.\"\n",
    ")\n",
    "agent = create_agent(model, tools, system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b035695a",
   "metadata": {},
   "source": [
    "## 11. Consulta al agente\n",
    "\n",
    "En este paso se realiza una **pregunta al agente** para verificar su funcionamiento.  \n",
    "El agente debe usar la herramienta de recuperaci√≥n de contexto para buscar informaci√≥n relevante en el texto previamente cargado y responder con base en ella.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78ddc25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the standard method for Task Decomposition?\n",
      "\n",
      "Once you get the answer, look up common extensions of that method.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (5412a908-164c-453e-9894-df915905533f)\n",
      " Call ID: 5412a908-164c-453e-9894-df915905533f\n",
      "  Args:\n",
      "    query: standard method for Task Decomposition\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into ‚ÄúProblem PDDL‚Äù, then (2) requests a classical planner to generate a PDDL plan based on an existing ‚ÄúDomain PDDL‚Äù, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    \"What is the standard method for Task Decomposition?\\n\\n\"\n",
    "    \"Once you get the answer, look up common extensions of that method.\"\n",
    ")\n",
    "\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acb4eba",
   "metadata": {},
   "source": [
    "## 12. Incorporar contexto din√°mico al agente\n",
    "\n",
    "En este paso se implementa una **middleware** que a√±ade contexto de manera autom√°tica a las consultas del usuario.  \n",
    "Esto mejora la precisi√≥n del modelo, ya que cada vez que se formula una pregunta, el agente busca informaci√≥n relevante en la base de documentos y la incorpora al *prompt* antes de generar la respuesta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e99ff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "@dynamic_prompt\n",
    "def prompt_with_context(request: ModelRequest) -> str:\n",
    "    \"\"\"Inject context into state messages.\"\"\"\n",
    "    last_query = request.state[\"messages\"][-1].text\n",
    "    retrieved_docs = vector_store.similarity_search(last_query)\n",
    "\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    system_message = (\n",
    "        \"You are a helpful assistant. Use the following context in your response:\"\n",
    "        f\"\\n\\n{docs_content}\"\n",
    "    )\n",
    "\n",
    "    return system_message\n",
    "\n",
    "\n",
    "agent = create_agent(model, tools=[], middleware=[prompt_with_context])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bfd0ca",
   "metadata": {},
   "source": [
    "## 13. Prueba del agente con contexto din√°mico\n",
    "\n",
    "En este paso se pone a prueba el nuevo agente, que ahora incorpora autom√°ticamente contexto relevante de la base de conocimiento cada vez que recibe una pregunta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "462a83dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is task decomposition?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Task decomposition is the process of breaking down a complex task into smaller, more manageable sub-tasks. This is a crucial step in planning and executing complicated actions, as it makes the overall objective easier to understand and achieve.\n",
      "\n",
      "There are several ways task decomposition can be performed:\n",
      "\n",
      "*   **By LLM with simple prompting:** This involves using prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\" to guide a Large Language Model (LLM) to break down a task.\n",
      "*   **By using task-specific instructions:** For certain types of tasks, specific instructions can be given. For example, for writing a novel, an instruction like \"Write a story outline\" would initiate task decomposition.\n",
      "*   **With human inputs:** A human can directly provide the decomposed steps for a task.\n",
      "*   **LLM+P (using external planners):** This approach involves an LLM translating a problem into a format understandable by a classical planner (like Planning Domain Definition Language or PDDL). The external planner then generates a plan, which the LLM translates back into natural language. This method outsources the planning step to a specialized tool.\n",
      "\n",
      "Techniques like **Chain of Thought (CoT)** and **Tree of Thoughts (ToT)** are advanced methods used by LLMs to perform task decomposition. CoT encourages the model to \"think step by step,\" while ToT explores multiple reasoning paths at each step, creating a tree-like structure of potential solutions.\n",
      "\n",
      "Effective task decomposition is essential for overcoming challenges in long-term planning, especially for LLMs which can struggle with adjusting plans when unexpected errors occur.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is task decomposition?\"\n",
    "for step in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
